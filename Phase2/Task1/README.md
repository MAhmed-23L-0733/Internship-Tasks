ğŸ“° News Classification with BERT

A machine learning project that fine-tunes BERT (Bidirectional Encoder Representations from Transformers) for automated news article classification using the AG News dataset.

ğŸ¯ Project Overview

This project implements a state-of-the-art text classification system that can automatically categorize news articles into four distinct categories:

- **World News** ğŸŒ
- **Sports** âš½
- **Business** ğŸ’¼
- **Science/Technology** ğŸ”¬

ğŸ”§ Technical Stack

- **Model**: BERT-base-uncased (Google's pre-trained transformer)
- **Framework**: Hugging Face Transformers
- **Dataset**: AG News (120,000 training samples, 7,600 test samples)
- **Metrics**: Accuracy and weighted F1-score
- **Training**: Fine-tuning with custom training arguments

ğŸ“Š Dataset Information

The AG News dataset contains news articles with four categories:

- Total training samples: 120,000
- Total test samples: 7,600
- Text preprocessing: Tokenization with 128 max sequence length
- Label encoding: Integer labels (0-3) for four categories

ğŸš€ Getting Started

Prerequisites

```bash
pip install transformers
pip install datasets
pip install torch
pip install scikit-learn
pip install numpy
```

Running the Project

1. **Open the Jupyter notebook**: `Task1_P2.ipynb`
2. **Run all cells sequentially** to:
   - Load and explore the AG News dataset
   - Tokenize text data using BERT tokenizer
   - Configure training parameters
   - Fine-tune the BERT model
   - Evaluate model performance
   - Save the trained model

Training Configuration

```python
- Learning Rate: 2e-5
- Batch Size: 16 (train/eval)
- Epochs: 3
- Weight Decay: 0.01
- Max Sequence Length: 128
- Evaluation Strategy: Per epoch
```

ğŸ“ˆ Model Performance

The model is evaluated using:

- **Accuracy**: Overall classification accuracy
- **F1-Score**: Weighted F1-score for multi-class performance
- **Best Model Selection**: Based on accuracy metric

ğŸ’¾ Model Output

After training, the model and tokenizer are saved to:

```
./news_classifier/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ vocab.txt
```

ğŸ” Key Features

- **Pre-trained BERT**: Leverages Google's BERT-base-uncased model
- **Fine-tuning**: Custom fine-tuning for news classification task
- **Efficient Training**: Optimized training arguments for best performance
- **Model Persistence**: Saves trained model for future inference
- **Comprehensive Evaluation**: Multiple metrics for performance assessment

ğŸ“ Usage Example

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

Load the trained model
model = AutoModelForSequenceClassification.from_pretrained('./news_classifier')
tokenizer = AutoTokenizer.from_pretrained('./news_classifier')

Classify a news article
text = "Apple Inc. reported strong quarterly earnings..."
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs)
predicted_class = outputs.logits.argmax().item()
```

ğŸ—ï¸ Project Structure

```
ğŸ“ Internship Tasks/
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â””â”€â”€ Task1_P2.ipynb          Main training notebook
â”œâ”€â”€ ğŸ“ ag_news_bert_results/    Training outputs and logs
â”œâ”€â”€ ğŸ“ news_classifier/         Saved model and tokenizer
â””â”€â”€ README.md                   This file
```

ğŸ“ Learning Outcomes

This project demonstrates:

- **Transfer Learning**: Using pre-trained BERT for domain-specific tasks
- **Text Classification**: Multi-class classification techniques
- **Model Fine-tuning**: Advanced training strategies with Hugging Face
- **Performance Evaluation**: Comprehensive model assessment
- **Model Deployment**: Saving and loading trained models

ğŸ”„ Next Steps

Potential improvements and extensions:

- Implement cross-validation for robust evaluation
- Experiment with different BERT variants (RoBERTa, DistilBERT)
- Add model interpretability and attention visualization
- Deploy model as a REST API or web application
- Implement real-time news classification pipeline

ğŸ“š References

- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [AG News Dataset](https://paperswithcode.com/dataset/ag-news)

---

**Author**: Internship Project  
**Last Updated**: September 2025  
**Status**: Complete âœ…
